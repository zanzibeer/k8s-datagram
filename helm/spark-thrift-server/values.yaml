# Default values for spark-thrift.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: apache/spark
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "v3.3.0"

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# FileSystem path to read logs from
# Example: s3a://bucket-name/spark/logs
logPath: ""

extraSparkConfigs:
  - "spark.thrift.fs.cleaner.enabled=true"

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext:
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  runAsUser: 65534

service:
  type: ClusterIP
  port: 80

ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: ""
      paths:
        - path: /
          pathType: Prefix
          
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

hadoopConfig:
  hiveSite: 
    ## Set config key and value pairs, e.g.
    # fs.defaultFS: "hdfs://namenode:8020"
    #hive.metastore.uris: "thrift://hive-metastore:9083"
    #hive.metastore.warehouse.dir: "hdfs://namenode:8020"
    hive.server2.transport.mode: "http"
    hive.server2.thrift.http.port: "10000"

sparkDefaultsConfig: 
  ## Set config key and value pairs, e.g.
  #spark.kubernetes.namespace: "spark-thrift"
  #spark.driver.host: "spark-thrift-server"
  spark.hadoop.hive.metastore.uris: "thrift://hive-metastore.apache-hive.svc.cluster.local:9083"
  spark.kubernetes.container.image: "apache/spark:v3.3.0"
  spark.driver.bindAddress: "0.0.0.0"
  spark.driver.port: "22321"
  spark.blockManager.port: "22322"
  spark.master: "k8s://https://kubernetes.default.svc"
  spark.sql.catalogImplementation: "hive"


# Enable RBAC (default on most clusters these days)
rbac:
  # Specifies whether RBAC resources should be created
  create: true


autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

nodeSelector: {}

tolerations: []

affinity: {}
